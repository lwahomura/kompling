{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Keyword_extraction.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565IWbhaQeJk",
        "colab_type": "text"
      },
      "source": [
        "## Извлечение ключевых слов\n",
        "\n",
        "Или keyword extraction - это упрощенный вариант задачи саммаризации (иногда это называется реферирование, но мне не очень нравится), т.е. извлечения главной информации из текста. Настоящая саммаризация подразумевает, что главная информация описывается нормальными полными предложениями, но сделать это очень сложно. Извлечь только основные слова проще и задачу решает тоже достаточно хорошо (по нескольким слова сразу приблизительно понятно о чем текст и читать ключевые слова сильно быстрее, чем даже самое хороше саммари). Еще одно преимущество ключевых слов - это то, что их удобно использовать в стандартном поиске (который работает только со словами и не анализирует последовательности). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoiNmzmZQeJl",
        "colab_type": "text"
      },
      "source": [
        "Одному тексту разные люди могут приписать разные ключевые слова, поэтому оценивать извлечение ключевых слов сложнее, чем обычную классификации. Нужно либо для каждого текста иметь несколько набор ключевых слов для каждого текста, либо брать тексты с ключевыми словами из разных источников. В любом случае с метрикам нужно быть аккуратными и смотреть больше на соотношение, чем на абсолютные значения. Ну и проверять полученные алгоритмы в реальной задаче или хотя бы на глаз. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8rBpiwXQjb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymorphy2[fast]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8D0EqUQRKkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fb4d7674-da32-43a1-8dad-a1e3b6a59539"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYBfwIdAQeJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json, os\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "morph = MorphAnalyzer()\n",
        "stops = set(stopwords.words('russian'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2Y4LqiQeJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGfI34hQeJs",
        "colab_type": "text"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QivK7mb-QeJu",
        "colab_type": "text"
      },
      "source": [
        "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUvKTQ7JRQ9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/mannefedov/ru_kw_eval_datasets/archive/master.zip && unzip master.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKphyjN9WyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd ru_kw_eval_datasets-master && mv data ../data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmtg69t_QeJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# скачаем данные в папке data и распакуем их\n",
        "PATH_TO_DATA = './data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRahLGjQeJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMxGxrhBQeJ2",
        "colab_type": "text"
      },
      "source": [
        "Объединим файлы в один датасет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tziCWJ_-QeJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4IHAMsQeJ6",
        "colab_type": "code",
        "outputId": "4720e91b-a91e-4505-a3ac-c3cc2d1eaae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17266, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbTD2U6cQeJ-",
        "colab_type": "code",
        "outputId": "ed25ea42-0553-4880-d344-e4b542a6adcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[армия, беспилотники, ввс, военная доктрина, военная техника, новые технологии, оружие, россия, самолет, армия]</td>\n",
              "      <td>Беспилотное будущее: в России разрабатываются конвертоплан и тяжёлый БПЛА</td>\n",
              "      <td>В России для нужд военных разрабатываются конвертопланы и тяжёлые беспилотные летательные аппараты. Об этом на авиакосмическом салоне МАКС-2017 заявил главком Воздушно-космических сил (ВКС) России Виктор Бондарев. По его словам, в будущем военные беспилотники смогут встраиваться в единую систему управления, что позволит существенно снизить расходы на подготовку операторов. В беседе с RT эксперты отметили, что российские инженеры поставили перед собой весьма непростую задачу.</td>\n",
              "      <td>https://russian.rt.com/russia/article/411398-rossiya-konvertoplat-razrabotka</td>\n",
              "      <td>Главком ВКС России генерал-полковник Виктор Бондарев заявил, что в стране в интересах военных ведётся разработка конвертопланов и тяжёлых беспилотников. По словам генерала, в будущем беспилотники смогут встраиваться в единую систему управления, что позволит существенно снизить расходы на подготовку операторов этих летательных аппаратов.\\n Также по теме \\n Удар «медведей»: российские Ту-95МС поразили объекты ИГ в Сирии новейшими крылатыми ракетами \\n Стратегические ракетоносцы Ту-95МС («медведь» по классификации НАТО) взлетели с российского аэродрома Энгельс в Саратовской области и,... \\n«Беспилотная тематика развивается. Как бы то ни было, обучить лётчика дороже, чем поставить на самолет хороший автопилот. Если уже сейчас оператор может управлять одним-двумя беспилотниками, то со временем, с развитием наземной составляющей, он сможет управлять пятью, а то и десятью аппаратами, поэтому это будет ещё дешевле», — заявил Бондарев.\\nКроме того, он отметил, что беспилотная авиация в буду...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[аргентина, барселона, в россии, в мире, испания, лионель месси, россия, сборная россии по футболу, спорт, спортсмен, тренер, фк зенит, футбол, чемпионат россии, чемпионат мира по футболу 2018 в россии, эксклюзив rt, лига чемпионов уефа, футбол, чм по футболу 2018]</td>\n",
              "      <td>«Россия достойно выступит на чемпионате мира»: футболист «Зенита» Краневиттер о ЧМ-2018, Месси и жизни в Петербурге</td>\n",
              "      <td>Сборная России сможет проявить себя на домашнем чемпионате мира. Об этом в интервью RT заявил полузащитник «Зенита» Матиас Краневиттер. 24-летний аргентинец назвал звезду мирового футбола Лионеля Месси обычным человеком, рассказал о своём прозвище на заре карьеры, а также признался, что получает удовольствие от пребывания в Санкт-Петербурге и «Зените».</td>\n",
              "      <td>https://russian.rt.com/sport/article/463504-kranevitter-intervyu-zenit-messi-rossiya</td>\n",
              "      <td>«Когда моя семья жила бедно, мне приходилось работать кедди»\\n— Расскажите о вашем лучшем футбольном воспоминании.\\n— Их очень много. Обычно первое, что приходит на ум, — это как ты ещё ребёнком мечтаешь попасть в профессиональный футбол. Играешь с большим удовольствием, сначала для тебя это как развлечение, а потом оно превращается в ежедневную работу. Мне стоило огромных усилий достичь того уровня, на котором я нахожусь сегодня.\\n— Вы ещё увлекаетесь гольфом. Как так получилось? \\nЯ живу в городе Йерба-Буэна. Вокруг бесконечные поля для игры в гольф. Когда я был мальчишкой, моя семья жила довольно бедно, мне приходилось работать кедди (помощником, который носит инвентарь для игры в гольф. — \\nRT\\n). Мне всегда это нравилось, я часто играл с друзьями, с родственниками. Этот спорт мне дал очень много. Сейчас я тоже иногда играю, если появляется свободное время. Гольф очень помог мне в своё время. И я думаю, что он во многом помог моей семье, когда я был подростком.\\nПубликация от M...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[армия, безопасность, владимир путин, высокие технологии, информационная война, минобороны, новые технологии, оборона, президент, россия, сергей шойгу, армия]</td>\n",
              "      <td>«Нам удалось вырваться вперёд»: как Россия превзошла Запад в автоматизации управления обороной</td>\n",
              "      <td>Три года назад в России начал работу Национальный центр управления обороной РФ (НЦУО). Структура в составе Минобороны позволила увеличить скорость обмена информацией и как результат —\\n сократить время принятия решений и цикл боевого управления. По мнению экспертов, России удалось опередить зарубежных партнёров в сфере автоматизации процессов управления обороной, а попытки Запада создать единый механизм взаимодействия военных и гражданских не увенчались успехом. О задачах и возможностях НЦУО — в материале RT.</td>\n",
              "      <td>https://russian.rt.com/russia/article/454993-centr-oborony-rossia</td>\n",
              "      <td>1 декабря 2014 года на боевое дежурство заступил Национальный центр управления обороной Российской Федерации (НЦУО РФ), расположившийся в здании Минобороны. Уникальная информационно-аналитическая структура была создана менее чем за год.\\nНЦУО объединил функции\\n Центрального командного пункта Генштаба и Ситуационного центра Минобороны, но функционирует на новой технологической платформе.\\nПрограммно-аппаратный комплекс (ПАК) на Фрунзенской набережной превосходит стоящий в Пентагоне суперкомпьютер по показателям производительности (16 против 5 петафлопс) и по возможностям хранения данных (236 против 12 петабайт).\\nПАК является разработкой «Объединённой приборостроительной корпорации». Как отмечает Минобороны, машина способна обрабатывать гигантские объёмы информации, моделировать развитие событий и рассчитывать наиболее оптимальные варианты выполнения тех или иных задач.\\n Первая дежурная смена Национального центра управления обороной РФ \\n © Минобороны России \\nПо выражению главы в...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                    keywords  ... abstract\n",
              "0                                                                                                                                                            [армия, беспилотники, ввс, военная доктрина, военная техника, новые технологии, оружие, россия, самолет, армия]  ...      NaN\n",
              "1  [аргентина, барселона, в россии, в мире, испания, лионель месси, россия, сборная россии по футболу, спорт, спортсмен, тренер, фк зенит, футбол, чемпионат россии, чемпионат мира по футболу 2018 в россии, эксклюзив rt, лига чемпионов уефа, футбол, чм по футболу 2018]  ...      NaN\n",
              "2                                                                                                             [армия, безопасность, владимир путин, высокие технологии, информационная война, минобороны, новые технологии, оборона, президент, россия, сергей шойгу, армия]  ...      NaN\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHFsu7MqQeKB",
        "colab_type": "text"
      },
      "source": [
        "Каждой статье приписано какое-то количество ключевых слов. **Наша задача - придумать как извлекать точно такой же список автоматически.**\n",
        "Зададим несколько метрик, по которым будем определять качество извлекаемых ключевых слов - точность, полноту, ф1-меру и меру жаккарда."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XInoe-BzQeKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(true_kws, predicted_kws):\n",
        "    assert len(true_kws) == len(predicted_kws)\n",
        "    \n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1s = []\n",
        "    jaccards = []\n",
        "    \n",
        "    for i in range(len(true_kws)):\n",
        "        \n",
        "        true_kw = set(true_kws[i])\n",
        "        predicted_kw = set(predicted_kws[i])\n",
        "        \n",
        "        tp = len(true_kw & predicted_kw)\n",
        "        union = len(true_kw | predicted_kw)\n",
        "        fp = len(predicted_kw - true_kw)\n",
        "        fn = len(true_kw - predicted_kw)\n",
        "        \n",
        "        if (tp+fp) == 0:\n",
        "            prec = 0\n",
        "        else:\n",
        "            prec = tp / (tp + fp)\n",
        "        \n",
        "        if (tp+fn) == 0:\n",
        "            rec = 0\n",
        "        else:\n",
        "            rec = tp / (tp + fn)\n",
        "        if (prec+rec) == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = (2*(prec*rec))/(prec+rec)\n",
        "          \n",
        "        if union == 0:\n",
        "          jac = 0\n",
        "        else:\n",
        "          jac = tp / union\n",
        "        \n",
        "        precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        f1s.append(f1)\n",
        "        jaccards.append(jac)\n",
        "    print('Precision - ', round(np.mean(precisions), 2))\n",
        "    print('Recall - ', round(np.mean(recalls), 2))\n",
        "    print('F1 - ', round(np.mean(f1s), 2))\n",
        "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
        "    \n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSoeRodQQeKD",
        "colab_type": "text"
      },
      "source": [
        "Проверим, что всё работает как надо."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZSdCw9QeKE",
        "colab_type": "code",
        "outputId": "a671dda7-4dba-40bd-81e3-618e38c324f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "evaluate(data['keywords'], data['keywords'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  1.0\n",
            "Recall -  1.0\n",
            "F1 -  1.0\n",
            "Jaccard -  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJvIsrF5QeL2",
        "colab_type": "text"
      },
      "source": [
        "## Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1fLoMkQeL3",
        "colab_type": "text"
      },
      "source": [
        "В семинаре использовался только небольшой кусочек данных. На всех данных пересчитайте baseline (tfidf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-RQAH9tQeL3",
        "colab_type": "text"
      },
      "source": [
        "**Ваша задача - предложить 3 способа побить бейзлайн на всех данных.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxILhmlkQeL4",
        "colab_type": "text"
      },
      "source": [
        "Нет никаких ограничений кроме:\n",
        "\n",
        "1) нельзя изменять метрику  \n",
        "2) решение должно быть воспроизводимым  \n",
        "3) способы дожны отличаться друг от друга не только гиперпараметрами (например, нельзя три раза поменять гиперпарамтры в TfidfVectorizer и сдать работу)  \n",
        "4) изменение количества извлекаемых слов не является улучшением (выберите одно значение и используйте только его)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFLx9sufQeL4",
        "colab_type": "text"
      },
      "source": [
        "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - https://forms.gle/GWzewBEpw8qnkv8t8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4eOU381QeL4",
        "colab_type": "text"
      },
      "source": [
        "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRJVvNaEQeL4",
        "colab_type": "text"
      },
      "source": [
        "Можно использовать мой код как основу, а можно придумать что-то полностью другое."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r1AgKBbQeL5",
        "colab_type": "text"
      },
      "source": [
        "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTKNAp7yQeL5",
        "colab_type": "text"
      },
      "source": [
        "В поисках идей можно почитать обзоры по теме (посмотрите еще статьи, в которых цитируются эти обзоры): https://www.semanticscholar.org/search?year%5B0%5D=2012&year%5B1%5D=2020&publicationType%5B0%5D=Reviews&q=keyword%20extraction&sort=relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h4qgUwyQeL5",
        "colab_type": "text"
      },
      "source": [
        "**Использовать доступные готовые решения тоже можно**. Так что погуглите перед тем, как приступать к заданию. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcULyRi9S_aU",
        "colab_type": "text"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MgHy8YJS1KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "morph = MorphAnalyzer()\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
        "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
        "\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8jnHOqqTHB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['content_norm'] = data['content'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ZLKrfPTSYu",
        "colab_type": "code",
        "outputId": "d15ab048-6d84-4489-8298-247a21382696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
        "tfidf.fit(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords.append([id2word[w] for w in top_inds])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.09\n",
            "Recall -  0.12\n",
            "F1 -  0.09\n",
            "Jaccard -  0.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbl0RALMd1Eq",
        "colab_type": "text"
      },
      "source": [
        "# Самое банальное(нормализация и векторизация)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGKww657fayf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install russian-names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PLu5foefwye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from russian_names import RussianNames\n",
        "some_names = set()\n",
        "for s in RussianNames(count=10000, output_type='dict').get_batch():\n",
        "  some_names.add(s['name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yShhdFVVgVsk",
        "colab_type": "code",
        "outputId": "6ea64435-00b5-4fd6-ec0a-cae0a28f8c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(some_names)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcj3a_Lvg-CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def updated_normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [word for word in words if len(word) > 3]\n",
        "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
        "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
        "    words = [word for word in words if not word in some_names]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UVLlUavhf4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['content_updated_norm'] = data['content'].apply(updated_normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf1mhFc9d0dt",
        "colab_type": "code",
        "outputId": "f8782505-7b37-43d4-f9ce-fe5b3769ff0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "ranges = [(1,1), (1,2), (2,2)]\n",
        "analyzers = ['word', 'char']\n",
        "\n",
        "data['content_updated_norm_str'] = data['content_updated_norm'].apply(' '.join)\n",
        "for r in ranges:\n",
        "  for a in analyzers:\n",
        "    print(r, a)\n",
        "    new_tfidf = TfidfVectorizer(ngram_range=r, min_df=5, analyzer=a)\n",
        "    new_tfidf.fit(data['content_updated_norm_str'])\n",
        "    new_id2word = {i:word for i,word in enumerate(new_tfidf.get_feature_names())}\n",
        "    new_texts_vectors = new_tfidf.transform(data['content_updated_norm_str'])\n",
        "    new_keywords = []\n",
        "\n",
        "    for row in range(new_texts_vectors.shape[0]):\n",
        "        new_row_data = new_texts_vectors.getrow(row)\n",
        "        new_top_inds = new_row_data.toarray().argsort()[0,:-11:-1]\n",
        "        new_keywords.append([new_id2word[w] for w in new_top_inds])\n",
        "\n",
        "    evaluate(data['keywords'], new_keywords)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1) word\n",
            "Precision -  0.09\n",
            "Recall -  0.12\n",
            "F1 -  0.09\n",
            "Jaccard -  0.05\n",
            "(1, 1) char\n",
            "Precision -  0.08\n",
            "Recall -  0.11\n",
            "F1 -  0.09\n",
            "Jaccard -  0.05\n",
            "(1, 2) char\n",
            "Precision -  0.0\n",
            "Recall -  0.0\n",
            "F1 -  0.0\n",
            "Jaccard -  0.0\n",
            "(2, 2) word\n",
            "Precision -  0.01\n",
            "Recall -  0.01\n",
            "F1 -  0.01\n",
            "Jaccard -  0.0\n",
            "(2, 2) char\n",
            "Precision -  0.0\n",
            "Recall -  0.0\n",
            "F1 -  0.0\n",
            "Jaccard -  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HIPzHAjD_EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "30a12eba-06d3-4151-f851-5da30c78341c"
      },
      "source": [
        "ranges = [(1,1), (1,2), (2,2)]\n",
        "analyzers = ['word', 'char']\n",
        "\n",
        "for r in ranges:\n",
        "  for a in analyzers:\n",
        "    print(r, a)\n",
        "    new_tfidf = TfidfVectorizer(ngram_range=r, min_df=5, analyzer=a)\n",
        "    new_tfidf.fit(data['content_norm_str'])\n",
        "    new_id2word = {i:word for i,word in enumerate(new_tfidf.get_feature_names())}\n",
        "    new_texts_vectors = new_tfidf.transform(data['content_norm_str'])\n",
        "    new_keywords = []\n",
        "\n",
        "    for row in range(new_texts_vectors.shape[0]):\n",
        "        new_row_data = new_texts_vectors.getrow(row)\n",
        "        new_top_inds = new_row_data.toarray().argsort()[0,:-11:-1]\n",
        "        new_keywords.append([new_id2word[w] for w in new_top_inds])\n",
        "\n",
        "    evaluate(data['keywords'], new_keywords)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1) word\n",
            "Precision -  0.1\n",
            "Recall -  0.13\n",
            "F1 -  0.1\n",
            "Jaccard -  0.06\n",
            "(1, 1) char\n",
            "Precision -  0.0\n",
            "Recall -  0.0\n",
            "F1 -  0.0\n",
            "Jaccard -  0.0\n",
            "(1, 2) word\n",
            "Precision -  0.09\n",
            "Recall -  0.12\n",
            "F1 -  0.09\n",
            "Jaccard -  0.05\n",
            "(1, 2) char\n",
            "Precision -  0.0\n",
            "Recall -  0.0\n",
            "F1 -  0.0\n",
            "Jaccard -  0.0\n",
            "(2, 2) word\n",
            "Precision -  0.01\n",
            "Recall -  0.01\n",
            "F1 -  0.01\n",
            "Jaccard -  0.0\n",
            "(2, 2) char\n",
            "Precision -  0.0\n",
            "Recall -  0.0\n",
            "F1 -  0.0\n",
            "Jaccard -  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkrwey1pSQG",
        "colab_type": "text"
      },
      "source": [
        "Дополнительная нормализация оказалась лишней.\n",
        "Лучший и победивший бейзалйн набор параметров векторайзера:  \n",
        "ngram_range=(1, 1), analyzer=word\n",
        " \n",
        "\n",
        "Precision -  0.1  \n",
        "Recall -  0.13  \n",
        "F1 -  0.1  \n",
        "Jaccard -  0.06  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ziG5MsXUomK",
        "colab_type": "text"
      },
      "source": [
        "# Опять пробуем графы?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J4K6-UwUrJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "def build_matrix(text, window_size=5):\n",
        "    vocab = set(text)\n",
        "    word2id = {w:i for i, w in enumerate(vocab)}\n",
        "    id2word = {i:w for i, w in enumerate(vocab)}\n",
        "    # преобразуем слова в индексы для удобства\n",
        "    ids = [word2id[word] for word in text]\n",
        "\n",
        "    # создадим матрицу совстречаемости\n",
        "    m = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    # пройдемся окном по всему тексту\n",
        "    for i in range(0, len(ids), window_size):\n",
        "        window = ids[i:i+window_size]\n",
        "        # добавим единичку всем парам слов в этом окне\n",
        "        for j, k in combinations(window, 2):\n",
        "            # чтобы граф был ненаправленный \n",
        "            m[j][k] += 1\n",
        "            m[k][j] += 1\n",
        "    \n",
        "    return m, id2word\n",
        "\n",
        "def some_centrality_measure(text, window_size=5, topn=5):\n",
        "    \n",
        "    matrix, id2word = build_matrix(text, window_size)\n",
        "    G = nx.from_numpy_array(matrix)\n",
        "    # тут можно поставить любую метрику\n",
        "    # менять тут \n",
        "    node2measure = dict(nx.degree_centrality(G))\n",
        "    \n",
        "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztru-KrcUzOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StxGKCiqU7E9",
        "colab_type": "code",
        "outputId": "d138fc12-4a06-43d4-dc30-e0f2b0ba4efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.1\n",
            "Recall -  0.13\n",
            "F1 -  0.1\n",
            "Jaccard -  0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIyag2qSVq-v",
        "colab_type": "text"
      },
      "source": [
        "Ура?..  \n",
        "Попробуем другие метрики  \n",
        "Upd: почему-то ОЧЕНЬ долго считаются, стало грустно и я перестал ждать"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "max596uOVUk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def certain_centrality_measure(measure, text, window_size=5, topn=5):\n",
        "    \n",
        "    matrix, id2word = build_matrix(text, window_size)\n",
        "    G = nx.from_numpy_array(matrix)\n",
        "    # тут можно поставить любую метрику\n",
        "    # менять тут \n",
        "    node2measure = dict(measure(G))\n",
        "    \n",
        "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ-UXnCXWwUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "measures = [\n",
        "            nx.betweenness_centrality,\n",
        "            nx.current_flow_closeness_centrality\n",
        "]\n",
        "for m in measures:\n",
        "  keyword_nx_new = data['content_norm'].apply(lambda x: certain_centrality_measure(m, x, 10, 10))\n",
        "  evaluate(data['keywords'], keyword_nx_new)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p5KlZa4uwP_",
        "colab_type": "text"
      },
      "source": [
        "# Что-то еще?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmkzrC1uxKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['title_norm'] = data['title'].apply(normalize)\n",
        "data['title_norm_str'] = data['title_norm'].apply(' '.join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5IVxP0__eXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def union_and_intersection(a,b):\n",
        "  if len(a) == 0:\n",
        "    return b, b\n",
        "  if len(b) == 0:\n",
        "    return a, a\n",
        "  return list(set(a)|set(b)), list(set(a)&set(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOCXDHvu1iEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
        "\n",
        "tfidf.fit(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "keywords_content = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords_content.append([id2word[w] for w in top_inds])\n",
        "\n",
        "tfidf.fit(data['title_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "texts_vectors = tfidf.transform(data['title_norm_str'])\n",
        "keywords_title = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords_title.append([id2word[w] for w in top_inds])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mvpduI3J2R7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7d3de861-ed5d-4b4e-d77c-faec589e37d6"
      },
      "source": [
        "unions, intersections = [], []\n",
        "for i in range(len(keywords_content)):\n",
        "  un,inter = union_and_intersection(keywords_content[i],keywords_title[i])\n",
        "  unions.append(un)\n",
        "  intersections.append(inter)\n",
        "\n",
        "evaluate(data['keywords'], unions)\n",
        "evaluate(data['keywords'], intersections)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.06\n",
            "Recall -  0.15\n",
            "F1 -  0.08\n",
            "Jaccard -  0.04\n",
            "Precision -  0.18\n",
            "Recall -  0.04\n",
            "F1 -  0.07\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrjSy_6yM1gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=5, analyzer='word')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuFQTrqNUPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf.fit(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "keywords_content = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords_content.append([id2word[w] for w in top_inds])\n",
        "\n",
        "tfidf.fit(data['title_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
        "texts_vectors = tfidf.transform(data['title_norm_str'])\n",
        "keywords_title = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords_title.append([id2word[w] for w in top_inds])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcWYVAAQNW5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94d8e8ee-876c-47c7-d5c2-b3b746800fd3"
      },
      "source": [
        "unions, intersections = [], []\n",
        "for i in range(len(keywords_content)):\n",
        "  un,inter = union_and_intersection(keywords_content[i],keywords_title[i])\n",
        "  unions.append(un)\n",
        "  intersections.append(inter)\n",
        "\n",
        "evaluate(data['keywords'], unions)\n",
        "print()\n",
        "evaluate(data['keywords'], intersections)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.06\n",
            "Recall -  0.15\n",
            "F1 -  0.08\n",
            "Jaccard -  0.05\n",
            "\n",
            "Precision -  0.18\n",
            "Recall -  0.05\n",
            "F1 -  0.07\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D9KyTl1Q66b",
        "colab_type": "text"
      },
      "source": [
        "Бейзлайн был такой:\n",
        "- Precision -  0.09\n",
        "- Recall -  0.12\n",
        "- F1 -  0.09\n",
        "- Jaccard -  0.05\n",
        "\n",
        "Новые результаты в половине из метрик дают результат хуже, однако:\n",
        "- результат для объединений:\n",
        " - точность упала на 33%\n",
        " - полнота выросла на 25%\n",
        " - F-мера упала на 11%\n",
        " - мера Жаккара не изменилась\n",
        "- результат для пересечений:\n",
        " - точность выросла на 100%\n",
        " - полнота упала на 58%\n",
        " - F-мера упала на 12%\n",
        " - мера Жаккара упала на 20%\n",
        "\n",
        "Как мне кажется, можно утверждать, что бейзлайн был побежден пересечениями т.к. для ключевых слов самая важная метрика - точность; падение же остальных метрик не так существенно(полнота не так важна, лучше не поставить тег, чем поставить неверный)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0tZdNukTpMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_keywords(words):\n",
        "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
        "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
        "\n",
        "    return words\n",
        "    \n",
        "data['keywords_norm'] = data['keywords'].apply(normalize_keywords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYSt0D59UYuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "22e6d319-89fe-4ca8-f176-d394df7d1a0c"
      },
      "source": [
        "evaluate(data['keywords_norm'], unions)\n",
        "print()\n",
        "evaluate(data['keywords_norm'], intersections)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.08\n",
            "Recall -  0.24\n",
            "F1 -  0.11\n",
            "Jaccard -  0.06\n",
            "\n",
            "Precision -  0.23\n",
            "Recall -  0.07\n",
            "F1 -  0.1\n",
            "Jaccard -  0.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns11V9acUvVh",
        "colab_type": "text"
      },
      "source": [
        "Бейзлайн был такой:\n",
        "- Precision -  0.09\n",
        "- Recall -  0.12\n",
        "- F1 -  0.09\n",
        "- Jaccard -  0.05\n",
        "\n",
        "А вот нормализовав ключевые слова мы получаем результаты куда лучше:\n",
        "- результат для объединений:\n",
        " - точность упала на 11%\n",
        " - полнота выросла на 100%\n",
        " - F-мера выросла на 22%\n",
        " - мера Жаккара выросла на 20%\n",
        "- результат для пересечений:\n",
        " - точность выросла на 156%\n",
        " - полнота упала на 41%\n",
        " - F-мера упала на 88%\n",
        " - мера Жаккара выросла на 20%\n",
        "\n",
        " Но честно ли это? Можем ли мы так делать?.."
      ]
    }
  ]
}